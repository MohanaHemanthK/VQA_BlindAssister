# Visual Question Answering (VQA) for Accessibility

This repository contains the code and resources for building a Visual Question Answering (VQA) system, aimed at assisting visually impaired individuals. The project leverages deep learning models to answer questions about images, integrating both vision and natural language processing capabilities.

## Team Members
- Mohana Hemanth Kundurthi (Team Leader)
- Venkata Sai Sumanth Aketi (Team Member)
- Shiva Kumar Goud Mucharla (Team Member)

## Dataset Overview
Dataset we used is [**VQA 2.0**](https://visualqa.org/index.html).

- **Purpose:** The dataset is designed to train models to answer open-ended questions about images. These questions require an understanding of vision, language, and commonsense knowledge2.

- **Images:** It contains 265,016 images, including images from the COCO dataset and abstract scenes.

- **Questions:** Each image has at least 3 questions on average, with a total of 1,105,904 questions in the dataset.

- **Answers:** For each question, there are 10 ground truth answers and 3 plausible but likely incorrect answers.

- **Automatic Evaluation:** The dataset includes an automatic evaluation metric to assess the performance of models

## Features
- **Exploratory Data Analysis (EDA)**: Understand the dataset and prepare it for model training.
- **Model Training**: Implements a VGG19 and LSTM-based architecture for image and text processing.
- **Inference**: Includes a streamlined pipeline for deploying the trained model, featuring speech recognition for voice-based queries.
- **Model Evaluation**: Provides scripts to validate the model's performance using various metrics.

---

## File Overview
### `EDA&VQA_Training.ipynb`
- Conducts EDA and preprocessing of the dataset.
- Implements the VQA model using VGG19 for image feature extraction and LSTMs for text processing.
- Training pipeline with visualizations of loss and accuracy.

### `InferenceModelV3.ipynb`
- Prepares the trained model for inference.
- Integrates voice input using `SpeechRecognition` and `gtts (Google Text To Speech)` for user-friendly interaction.
- Generates responses to queries about given images.

### `Model_testing.ipynb`
- Tests the trained model on a separate validation dataset.
- Computes evaluation metrics such as accuracy, precision, recall, and F1 score.
- Includes visualizations for result interpretation.

---

## Installation
1. Clone this repository:
   ```bash
   git clone https://github.com/your-username/your-repository.git
   ```
2. Install dependencies: (if you running code in local)
   ```
   pip install requirements.txt
   ```
3. Ensure you have the necessary libraries installed:
- TensorFlow
- SpeechRecognition
- OpenCV
- Matplotlib

---

## Usage
### Training the Model
1. Open EDA&VQA_Training.ipynb.
2. Update paths to the dataset and other configurations.
3. Run the notebook to train the model.

### Testing the Model
1. Open Model_testing.ipynb.
2. Configure the path to your test dataset.
3. Run the notebook to view performance metrics and outputs.

### Running the inference
1. Open FinalInferenceModel.ipynb.
2. Load the trained model weights.
3. Provide access to microphone and webcam and then.

---
## Visualizations:
![image](https://github.com/user-attachments/assets/73a95b2a-19e5-49be-810e-f9297e46371f)
Training vs. Validation Loss: The training loss consistently decreases, indicating that the model is learning, while the validation loss stabilizes, suggesting diminishing improvement and potential overfitting after a certain point.
Training vs. Validation Accuracy: Training accuracy steadily improves, but validation accuracy plateaus, indicating the model generalizes less effectively beyond the training data after early epochs.

---
## Testing:
![image](https://github.com/user-attachments/assets/18226e94-d5c1-4a5f-a318-78236b480164)
These are the few images that I caputured in real-time and sent them to the model & checked how our custom trained model is performing.

---
## Results:

The Visual Question Answering (VQA) model was implemented using a combination of VGG19 for extracting image features and an LSTM model to process text-based queries. To enhance accessibility, particularly for visually impaired users, we incorporated speech recognition and text-to-speech technologies into the system. The SpeechRecognition library was used to convert spoken questions into text. This required fine-tuning the audio processing pipeline to minimize background noise and ensure accurate transcription of voice inputs. These text inputs were then passed to the VQA model to generate answers based on the provided image.

To complete the interaction loop, we utilized gTTS (Google Text-to-Speech) to vocalize the answers generated by the VQA model. This allowed users to receive spoken responses, making the system entirely hands-free. The gTTS output was further processed using pydub to fine-tune audio playback. By integrating these components, we created a multi-modal solution that seamlessly combines vision, natural language processing, and audio-based interaction, delivering an inclusive and user-friendly experience.

The Video provided below shows the final results.

**Sound ON**

https://github.com/user-attachments/assets/b58ffd22-717c-451d-a60a-687d2e7d0297

---
## References:
1. Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., & Parikh, D. (2017, May 15). Making the V in VQA matter: Elevating the role of image understanding in visual question answering. arXiv.org. https://arxiv.org/abs/1612.00837
2. Bordes, F., Pang, R. Y., Ajay, A., Li, A. C., Bardes, A., Petryk, S., Mañas, O., Lin, Z., Mahmoud, A., Jayaraman, B., Ibrahim, M., Hall, M., Xiong, Y., Lebensold, J., Ross, C., Jayakumar, S., Guo, C., Bouchacourt, D., Al-Tahan, H., … Chandra, V. (2024, May 27). An introduction to vision-language modeling. arXiv.org. https://arxiv.org/abs/2405.17247
3. Rotstein, N., Bensaid, D., Brody, S., Ganz, R., & Kimmel, R. (2023, November 15). FUSECAP: Leveraging large language models for enriched fused image captions. arXiv.org. https://arxiv.org/abs/2305.17718
4. Jiang, X., Zheng, J., Liu, R., Li, J., Zhang, J., Matthiesen, S., & Stiefelhagen, R. (2024, November 25). @Bench: Benchmarking vision-language models for human-centered assistive technology. arXiv.org. https://arxiv.org/abs/2305.17718
